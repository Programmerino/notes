Left: 9:18 AM
Arrived: 9:29 AM
Started: 9:30 AM

* Monte-Carl Reinforcement Learning
	* Waits until the terminal state and then values all the states leading up to it
		* This can be made more sparse by breaking tasks into smaller objectives
	* They are model-free
	* They are based on the idea that the value of a state should be the mean return based on that state
	* They situations need to eventually terminate
	* It learns from $k$ episodes of experience under the policy $\pi$
		* $k$ typically needs to be very large
* In First-Visit MC Policy Evaluation, the first time-step t that s is visited in an episodeâ€¦
	* Increment the counter for that state
	* Update G = R + $\gamma G$
	* Increment the total return by G
	* Estimate value by dividing total return by the counter
	* By the law of large numbers, the value function will converge to the optimal value function
* Vs. in Every-Visit MC Policy Evaluation which does that every time the state is visited.
* First-Visit MC Policy Evaluation is the most primitive form of RL