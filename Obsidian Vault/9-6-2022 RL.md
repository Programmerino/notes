Left: 9:17
Arrived: 9:29

* Optimal stochastic policies need to consider all the ways their action might happen, which can lead to different decisions than deterministic policies
* HW
	* Optimal is guaranteed getting the goal
	* Answer: Hug the left wall and the top wall
* Bandits are MDPs with one state
* Partially observable problems can be converted into MDPs
* A Markov process is a memoryless random process with the Markov property
* The Markov process is a tuple of a state and a dynamics/transition model
	* The Markov reward process adds a reward function and discount factor (0 to 1)
		* The value can be calculated with a simulation, or you can use the Markov property... SAVED UP TO HERE
			* The value function has an immediate reward and future reward component.
			* Expected value can be calculated from $\mathbb{E}\left[R_{t+1} + \gamma v(S_{t+1}) | S_t = s\right]$, which allows value to be calculated with $v(s) = R_s + \gamma \sum_{s' \in S} P_{ss'}v(s')$.
				* The P adds value to the transition probability which considers the above problem (see first bullet)
				* This can be represented
				* An analytical solution can be used by solving for v, but this requires inverting a matrix which is O(N^3) complexity
				* The iterative solution calculates each state's value over and over (with other states getting updated) until the values converge which has an O(|S|^2) complexity for each iteration.
			* The Markov Decision Process adds a set of actions
				* R can be interpreted as the reward of the current state, the reward of moving to a state using an action, or the value of an action in the current state
* A policy $\pi$ specifies which actions to take in each state which can be deterministic and stochastic
	* You can have deterministic policies in stochastic environments, etc.
	* It is the distribution of actions given states: $P[A_t = a | S_t = s]$
	* It fully defines the behavior of the agent
	* It is time independent since it depends on the current Markov state, and not on history
	* Given an MDP and a policy, the state sequence is a Markov Process <S, $P^\pi$>
	* The state and reward sequence is a Markov Reward Process <S, $P^\pi$, $R^\pi$, $\gamma$> where $p^\pi_{ss'} = \sum_{a\in A} \pi(a|s)p^a_{ss'}$ and $R_s^pi = \sum_{a\in A} \pi(a|s)R_s^a$
	* The state-value function $v_{pi}(s)$ of an MDP is the expected return starting from state s and following policy pi: $v_{\pi}(s) = E_pi[G_t|S_t = s]$
	* The action-value function $q_{\pi}(s)$ of an MDP is the expected return starting from state s, taking action a, and then following a policy $\pi$: $q_{\pi}(s, a) = E_{\pi}[G_t | S_t = s, A_t = a]
	* The Bellman Expectation Equation is the similar to the previous Bellman equation but considers actions
		* With this the value function is $v_{\pi}(s) = \sum_{a \in A} \pi(a | s)q_{\pi}(s, a)$ or $q_{\pi}(s, a) = R_s^a + \gamma\sum_{s' \in S} p_{ss'}^a v_{\pi}(s')$
* Horizon is the number of time steps in each episode which can be infinite
* Return is the total discounted reward from time-step t to horizon
* The discount is the present value of future reward
	* 0 leads to myopic evaluation here we only care about immediate reward while 1 leads to far-sighted evaluation
	* They are mathematically convenient since they avoid infinite returns and values and don't fully represent the future. There can also be situation-specific benefits to discount
* The value function gives the long-term return of a state.
* The optimal state-value function is the maximum state-value function over all policies and the optimal action-value function is the same for actions
* An MDP is solved once we know the optimal value function