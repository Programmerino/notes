SAVING

Left: 9:18
Arrived: 9:30
Started: 9:30

* The Bellman Expectation formula helps to estimate the value of a policy
* The optimal state-value function is the maximum state-value function over all policies, the same for action-value
* A policy is greater than another if, when used in a state-value function, always provides greater value than the other for any state
* There is always a deterministic optimal policy for any MDP
* $q_*$ is the state value of a state
* The Bellman Optimality Equation takes the action with the maximum value rather than taking the average of the actions our agent can take (Bellman Expectation Equation)
	* You take the cost of an action and add it to the state value of the state the action would take you to
	* Take the max of these
	* That is your state's value
* Dynamic programming is particularly useful whenâ€¦
	* Subproblems occur multiple times with reuse possible
	* Regardless of the initial conditions, the control is optimal for the remaining problem
* DP works well for MDPs, and is a form of AI planning
* An MRP is an MDP with a fixed policy
* Control means choosing the optimal action
	* This can be done by using the Bellman Expectation Equation and greedy policy improvement through policy iteration or through using the Bellman Optimality Equation and value iteration.
* Iterative Policy Evaluation uses the Bellman Expectation function where the recursive reference to itself is replaced by looking at the value function from the previous iteration. We continually iterate this way until we converge on a value.
	* This is sync backup, async backup can consider calculations which came from the same iteration
	* After this converges, we can form a better policy by choosing the action which leads to the best state under this method
		* The optimal policy typically converges much quicker than the value function
* 