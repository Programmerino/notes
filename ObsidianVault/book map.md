## CVEN 3227
**Mathematical Statistics**:
1.  Events, Venn diagrams: **Chapter 1.2 Sample Spaces**
2.  Fundamental axioms of probability: **Chapter 1.3 Probability Measures**
3.  Theorems of total probability and Bayes: **Chapter 1.5 Conditional Probability**
4.  Random variables and probability distributions: **Chapter 2 Random Variables**
5.  Expectation and moments: **Chapter 4 Expected Values**
6.  Multiple random variables: **Chapter 3 Joint Distributions**
7.  Binomial and geometric distributions: **Chapter 2.1.2 The Binomial Distribution and Chapter 2.1.3 The Geometric and Negative Binomial Distributions**
8.  Normal distribution: **Chapter 2.2.3 The Normal Distribution**
10.  Other distributions: Poisson, Uniform, Extreme values: **Chapter 2.1.5 The Poisson Distribution**
11.  Estimation of parameters: Method of moments: **Chapter 8.4 The Method of Moments**
12.  Estimation of parameters: Method of max. likelihood: **Chapter 8.5 The Method of Maximum Likelihood**
13.  Hypothesis testing: **Chapter 9 Testing Hypotheses and Assessing Goodness of Fit**
15.  Goodness of fit: **Chapter 9 Testing Hypotheses and Assessing Goodness of Fit**
18.  Regression: **Chapter 14 Linear Least Squares**
**Statistics**:
1.  Events, Venn diagrams: **Section 5.2 Events**
2.  Fundamental axioms of probability: **Section 5.3 Some Rules of Probability**
4.  Random variables and probability distributions: **Section 5.4 Discrete Random Variables and Probability Distributions**
7.  Binomial and geometric distributions: **5.6 The Binomial Distribution**
8.  Normal distribution: **Chapter 6 The Normal Distribution**
13.  Hypothesis testing: **Chapter 9 Hypothesis Tests for One Population Mean**
14.  Confidence intervals: **Chapter 8 Confidence Intervals for One Population Mean**
15.  Goodness of fit: **Section 12.2 Chi-Square Goodness-of-Fit Test**
18.  Regression: **Chapter 14 Inferential Methods in Regression and Correlation**
## MATH 2130
* [ ] Mar 06 - Section 4.1 - Vector Spaces and Subspaces
* [ ] Mar 08 - Section 4.2 - Null Spaces, Column Spaces, and Linear Transformations
* [ ] Mar 15 - Section 4.3 - Linearly Independent Sets; Bases
**Linear Algebra**:
1.  Systems of Linear Equations: **Section 3-3 or Section 3-4**
2.  Row Reduction and Echelon Forms: **Section 3-1**
6.  Linear Independence: **Section 1-5 or Section 5-2**
7.  Introduction to Linear Transformations: **Section 2-1**
8.  The Matrix of a Linear Transformation: **Section 2-2**
9.  Matrix Operations: **Section 2-3 or Section 3-1**
10.  The Inverse of a Matrix: **Section 2-4 or Section 3-2**
11.  Characterizations of Invertible Matrices: **Section 2-4**
12.  Subspaces of Rn: **Section 1-3 or Section 5-6**
13.  Dimension and Rank: **Section 1-6 or Section 5-6**
14.  Introduction to Determinants: **Section 4-1 or Section 4-2**
15.  Properties of Determinants: **Section 4-3 or Section 4-4**
17.  Vector Spaces and Subspaces: **Section 1-2 or Section 1-3**
19.  Linearly Independent Sets; Bases: **Section 1-5 or Section 1-6**
21.  The Dimension of a Vector Space: **Section 1-6 or Section 5-6**
23.  Change of Basis: **Section 2-5: The Change of Coordinate Matrix**
24.  Eigenvectors and Eigenvalues: **Section 5-1: Eigenvalues and Eigenvectors**
25.  The Characteristic Equation: **Section 5-4: Invariant Subspaces and The Cayley-Hamilton Theorem**
26.  Diagonalization: **Section 5-2: Diagonalizability**
27.  Eigenvectors and Linear Transformations: **Section 5-1: Eigenvalues and Eigenvectors**
28.  Inner Product, Length, and Orthogonality: **Section 6-1: Inner Products and Norms**
29.  Orthogonal Sets: **Section 6-2: The Gram-Schmidt Orthogonalization Process and Orthogonal Components**
30.  Orthogonal Projections: **Section 6-6: Orthogonal Projections and The Spectral Theorem**
31.  The Gram-Schmidt Process: **Section 6-2: The Gram-Schmidt Orthogonalization Process and Orthogonal Components**
32.  Least Squares Problems: **Section 6-7: The Singular Value Decomposition and The Pseudoinverse**
**Advanced Engineering Mathematics**:
1.  Systems of Linear Equations -> **Chapter 7.3 Linear Systems of Equations. Gauss Elimination**
2.  Row Reduction and Echelon Forms -> **Chapter 7.3 Linear Systems of Equations. Gauss Elimination**
3.  Vector Equations -> **Chapter 9.1 Vectors in 2-Space and 3-Space**
4.  The Matrix Equation Ax = b -> **Chapter 7.3 Linear Systems of Equations. Gauss Elimination**
5.  Solution Sets of Linear Systems -> **Chapter 7.5 Solutions of Linear Systems: Existence, Uniqueness**
6.  Linear Independence -> **Chapter 7.4 Linear Independence. Rank of a Matrix. Vector Space**
7.  Introduction to Linear Transformations -> **Chapter 7.9 Vector Spaces, Inner Product Spaces. Linear Transformations (Optional)**
8.  The Matrix of a Linear Transformation -> **Chapter 7.9 Vector Spaces, Inner Product Spaces. Linear Transformations (Optional)**
9.  Matrix Operations -> **Chapter 7.2 Matrix Multiplication**
10.  The Inverse of a Matrix -> **Chapter 7.8 Inverse of a Matrix. Gauss–Jordan Elimination**
11.  Characterizations of Invertible Matrices -> **Chapter 7.8 Inverse of a Matrix. Gauss–Jordan Elimination**
12.  Subspaces of Rn -> **Chapter 7.4 Linear Independence. Rank of a Matrix. Vector Space**
13.  Dimension and Rank -> **Chapter 7.4 Linear Independence. Rank of a Matrix. Vector Space**
14.  Introduction to Determinants -> **Chapter 7.6 For Reference: Second- and Third-Order Determinants**
15.  Properties of Determinants -> **Chapter 7.7 Determinants. Cramer’s Rule**
16. Cramer’s Rule, Volume, and Linear Transformations -> **Chapter 7.7 Determinants. Cramer’s Rule**
17. Vector Spaces and Subspaces -> **Chapter 7.9 Vector Spaces, Inner Product Spaces. Linear Transformations (Optional)**
1.  Change of Basis: This topic is related to **Eigenbases. Diagonalization. Quadratic Forms** in section **8.4**[1](https://brilliant.org/wiki/change-of-basis/)[2](https://www.statlect.com/matrix-algebra/change-of-basis).
2.  Eigenvectors and Eigenvalues: This topic is covered in **The Matrix Eigenvalue Problem. Determining Eigenvalues and Eigenvectors** in section **8.1**[3](https://en.wikipedia.org/wiki/Change_of_basis).
3.  The Characteristic Equation: This topic is also related to section **8.1**, as it is used to find the eigenvalues of a matrix[3](https://en.wikipedia.org/wiki/Change_of_basis).
4.  Diagonalization: This topic is also related to section **8.4**, as it involves finding a basis of eigenvectors and transforming a matrix into a diagonal form[1](https://brilliant.org/wiki/change-of-basis/)[2](https://www.statlect.com/matrix-algebra/change-of-basis).
5.  Eigenvectors and Linear Transformations: This topic is also related to section **8.4**, as it involves studying how linear transformations affect eigenvectors and eigenvalues[1](https://brilliant.org/wiki/change-of-basis/)[2](https://www.statlect.com/matrix-algebra/change-of-basis).
6.  Inner Product, Length, and Orthogonality: This topic is covered in **Inner Product (Dot Product)** in section **9.2**[3](https://en.wikipedia.org/wiki/Change_of_basis).
7.  Orthogonal Sets: This topic is also related to section **9.2**, as it involves finding sets of vectors that are mutually orthogonal[3](https://en.wikipedia.org/wiki/Change_of_basis).
8.  Orthogonal Projections: This topic is related to **Vector and Scalar Functions and Their Fields. Vector Calculus: Derivatives** in section **9.4**, as it involves projecting a vector onto another vector or a subspace[4](https://www.khanacademy.org/math/linear-algebra/alternate-bases/change-of-basis/v/linear-algebra-change-of-basis-matrix)[5](https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/linear-algebra/change-of-basis/).
9.  The Gram-Schmidt Process: This topic is also related to section **9.4**, as it involves finding an orthogonal basis for a subspace using inner products[4](https://www.khanacademy.org/math/linear-algebra/alternate-bases/change-of-basis/v/linear-algebra-change-of-basis-matrix)[5](https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/linear-algebra/change-of-basis/).
10.  Least Squares Problems: This topic is not directly covered in the table of contents, but it can be related to section **9.4**, as it involves finding the best approximation of a vector by another vector or a subspace using orthogonal projections[4](https://www.khanacademy.org/math/linear-algebra/alternate-bases/change-of-basis/v/linear-algebra-change-of-basis-matrix)[5](https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/linear-algebra/change-of-basis/).


